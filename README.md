# Product-Matching-Using-String-Similarity
## Summary
The main objective for this task is to take two different datasets from different companies and match two products that are likely to be the same. Two different techniques were used to do this, a naiive approach without blocking, and one using blocking. Overall, the naiive approach produced a recall value of 0.9060 and a precision value of 0.9000, and the blocking method produced a pair completeness value of 0.9881 and a reduction ratio of 0.9408.

## Naïve linkage without blocking
One of the most efficient and effective methods to link similar products together that was discovered was to take advantage of the fact that the large majority of products in Buy’s dataset contain the manufacture name for the item. Thus, the first technique that was used was to take the manufacture name from each item in Buy’s data set, and pair it with every item in Abt’s dataset that contains words similar to the manufacture name in its item name. 

All strings are pre-processed by converting it into lowercase. Most text comparison algorithms that I used in this task use either bigrams or trigrams as their given strings. I discovered that this massively improves the results of the algorithms, as similar items are more likely to share the same bigrams/trigrams with each other. Different string comparison algorithms were used at different stages as the chosen ones yielded the best results (all algorithms were considered for each stage). All threshold values for string comparisons were generated as a result of trial and error (whatever gave the best score) 

The similarity method that was used in the first step was textdistance’s Jaccard index algorithm on the bigrams of the item name from Abt and bigrams of the manufacturer name. Therefore, by using a nested for loop and collecting every combination that has a score above zero, I can discard products that are unlikely unrelated to that manufacturer. This collected set of data has a recall value of 0.9799 and precision of 0.0477. These pairs are then filtered for pairs which have a bigram name similarity of at least 61 using fuzzywuzzy’s token_set_ratio. The threshold of 61 was generated as it produced the best results after using trial and error.

Next, I noticed that a large majority of products in Abt’s data have a product code on the rightmost part of its name. Most of these product codes are found inside an item name within Buy’s data, therefore we can find a lot of matches with this information. For each pair generated in the previous step, it will parse the product code from Abt’s item name. Then, I flatten the name for Buy’s item by removing all whitespace, “-“ and “/” symbols. Finally, I collect the pairs that contain the same product code (without the last character as many matches’ product codes only differ by the last character). It will then attempt the do the same with a product code from Buy’s item, if it exists. This results in a data set with an incredible 0.8054 recall and 0.9756 precision. The pairs which could not match a product code are collected to be further inspected. For each unique item from Abt in this data set, only the pair with the highest token_set_ratio value from the previous step is selected.

Then, the algorithm looks for “special” words within the names of each item in the pair. Special words are defined as words that have at least one letter and number, or at least one number and one symbol (e.g. an apostrophe). These special words are place in a set, and if one of the sets of special words is a subset of the other set in the pair, then we recognise this pair as a match. If not, then put aside this pair for further analysis.

Finally, the leftover pairs have their names pre-processed by removing the product code from the right of the name, and are treated as a match if the string similarity using textdistance’s cosine algorithm for the trigrams of the names are greater than 0.504 (value generated from trial and error to get the best results)

Overall, this algorithm results in a recall value of 0.9060 and a precision value of 0.9000. It was relatively easy to get a recall value of 0.8389 and precision value of 0.9542 without a large reliance on string similarity, however I struggled to create an efficient algorithm to find the remaining pairs. One method that I believe could be used to solve this issue is using something like wordnet (a library that compares the similarities of the definition of words) to find similar items that don’t share words. Additionally, I could create an algorithm to strip meaningless words from the item names before comparing string similarity, which would improve the detected similarity between strings with a large length disparity. Furthermore, I could give certain words a larger ‘weight’ when comparing strings (e.g. ‘television’ should be weighted more than ‘white’). 

## Blocking for efficient data linkage
In order to create a linear algorithm which has a time complexity of O(a + b) where a is the number of items in Abt’s csv, and b is the number of items in Buy’s csv, each list of items is only gone through once. All strings are pre-processed by converting all text to lowercase.  Buy’s csv is parsed as we want to collect all of the unique manufacturer names before viewing Abt’s items. Each unique word in all of the manufacturer names is added to a set, and the item id that each word is associated with is created as a block. Next, it looks for the ‘special’ words in the name for each item using the algorithm that was used in the previous task, and creates a block with each special word and its associated item id.

Next, a similar process is used when Abt’s csv is being parsed. As was done with Buy, it collects every ‘special’ word in each item name and creates a block with the respective id, as what was done before. However, a different process is used for the manufacturer. As discovered in the previous task, manufacturer names are likely to be contained in the items in Abt. Since we already created a set with all manufacturer names discovered in Buy, we split the name into an array of words. For each word in the name, if the word is inside the manufacturer set, we create a block with the respective Abt id for the name. Since checking if a value exists in a set has a time complexity of O(1) (due to hashing), the entire algorithm still remains linear as we can approximate the average number of words in each name in Abt to be around 9.0731. Therefore, this algorithm still has a time complexity of O(a + b) for large values of a and b.

As a result, the algorithm produces a pair completeness value of 0.9881, and a reduction ratio value of 0.9408. I am extremely satisfied with these results as it produced amazing results in such a small algorithm. This algorithm runs a lot quicker than the naïve approach, however, this comes at the trade-off of having much larger output files as there are a lot more rows of output data. One improvement that could improve the results would be to incorporate the use of the descriptions of the items. There is probably important information inside the description which can be used to block the missing items together. Additionally, I could go through Abt and Buy and try to create a graph which maps the mutual information between words within the names of each item. E.g. the word “Samsung” would likely have a high correlation with words related to technology.
